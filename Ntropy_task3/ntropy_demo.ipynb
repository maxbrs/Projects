{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('~/git/Projects/Ntropy_task3/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTROPY : Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worldline has open sourced some of their data of credit card transactions to\n",
    "try to predict fraud: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "Imagine this dataset is cut in half horizontally. Alice has half of the data\n",
    "and Bob has the other half. Neither of them wants to send their raw data to us.\n",
    "However, we convince them to let our model learn from their data in a federated\n",
    "setting. Implement a way for our model to train on the combined data of both\n",
    "Alice and Bob without either of them sending us any raw data. Does your\n",
    "approach have a quantifiable \"privacy budget\" (some approaches do, others don't)?\n",
    "Compare the accuracy of this model with one which has access to all of the raw\n",
    "data at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-jngh48af because the default path (/home/maxime/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ntropy import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.449044</td>\n",
       "      <td>-1.176339</td>\n",
       "      <td>0.913860</td>\n",
       "      <td>-1.375667</td>\n",
       "      <td>-1.971383</td>\n",
       "      <td>-0.629152</td>\n",
       "      <td>-1.423236</td>\n",
       "      <td>0.048456</td>\n",
       "      <td>-1.720408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>0.313894</td>\n",
       "      <td>0.027740</td>\n",
       "      <td>0.500512</td>\n",
       "      <td>0.251367</td>\n",
       "      <td>-0.129478</td>\n",
       "      <td>0.042850</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.384978</td>\n",
       "      <td>0.616109</td>\n",
       "      <td>-0.874300</td>\n",
       "      <td>-0.094019</td>\n",
       "      <td>2.924584</td>\n",
       "      <td>3.317027</td>\n",
       "      <td>0.470455</td>\n",
       "      <td>0.538247</td>\n",
       "      <td>-0.558895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>0.238422</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.996710</td>\n",
       "      <td>-0.767315</td>\n",
       "      <td>-0.492208</td>\n",
       "      <td>0.042472</td>\n",
       "      <td>-0.054337</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.249999</td>\n",
       "      <td>-1.221637</td>\n",
       "      <td>0.383930</td>\n",
       "      <td>-1.234899</td>\n",
       "      <td>-1.485419</td>\n",
       "      <td>-0.753230</td>\n",
       "      <td>-0.689405</td>\n",
       "      <td>-0.227487</td>\n",
       "      <td>-2.094011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231809</td>\n",
       "      <td>-0.483285</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.161135</td>\n",
       "      <td>-0.354990</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>121.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.069374</td>\n",
       "      <td>0.287722</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>2.712520</td>\n",
       "      <td>-0.178398</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.096717</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>-0.221083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036876</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.071407</td>\n",
       "      <td>0.104744</td>\n",
       "      <td>0.548265</td>\n",
       "      <td>0.104094</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.021293</td>\n",
       "      <td>27.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.791855</td>\n",
       "      <td>-0.327771</td>\n",
       "      <td>1.641750</td>\n",
       "      <td>1.767473</td>\n",
       "      <td>-0.136588</td>\n",
       "      <td>0.807596</td>\n",
       "      <td>-0.422911</td>\n",
       "      <td>-1.907107</td>\n",
       "      <td>0.755713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151663</td>\n",
       "      <td>0.222182</td>\n",
       "      <td>1.020586</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>-0.232746</td>\n",
       "      <td>-0.235557</td>\n",
       "      <td>-0.164778</td>\n",
       "      <td>-0.030154</td>\n",
       "      <td>58.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.752417</td>\n",
       "      <td>0.345485</td>\n",
       "      <td>2.057323</td>\n",
       "      <td>-1.468643</td>\n",
       "      <td>-1.158394</td>\n",
       "      <td>-0.077850</td>\n",
       "      <td>-0.608581</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>-0.436167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>1.353650</td>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.065084</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>-0.087086</td>\n",
       "      <td>-0.180998</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>15.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.103215</td>\n",
       "      <td>-0.040296</td>\n",
       "      <td>1.267332</td>\n",
       "      <td>1.289091</td>\n",
       "      <td>-0.735997</td>\n",
       "      <td>0.288069</td>\n",
       "      <td>-0.586057</td>\n",
       "      <td>0.189380</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024612</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.103758</td>\n",
       "      <td>0.364298</td>\n",
       "      <td>-0.382261</td>\n",
       "      <td>0.092809</td>\n",
       "      <td>0.037051</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.436905</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.924591</td>\n",
       "      <td>-0.727219</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.707642</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>-0.665271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194796</td>\n",
       "      <td>-0.672638</td>\n",
       "      <td>-0.156858</td>\n",
       "      <td>-0.888386</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>-0.049027</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.131024</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.0</td>\n",
       "      <td>-5.401258</td>\n",
       "      <td>-5.450148</td>\n",
       "      <td>1.186305</td>\n",
       "      <td>1.736239</td>\n",
       "      <td>3.049106</td>\n",
       "      <td>-1.763406</td>\n",
       "      <td>-1.559738</td>\n",
       "      <td>0.160842</td>\n",
       "      <td>1.233090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503600</td>\n",
       "      <td>0.984460</td>\n",
       "      <td>2.458589</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>-0.481631</td>\n",
       "      <td>-0.621272</td>\n",
       "      <td>0.392053</td>\n",
       "      <td>0.949594</td>\n",
       "      <td>46.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.492936</td>\n",
       "      <td>-1.029346</td>\n",
       "      <td>0.454795</td>\n",
       "      <td>-1.438026</td>\n",
       "      <td>-1.555434</td>\n",
       "      <td>-0.720961</td>\n",
       "      <td>-1.080664</td>\n",
       "      <td>-0.053127</td>\n",
       "      <td>-1.978682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177650</td>\n",
       "      <td>-0.175074</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.295814</td>\n",
       "      <td>0.332931</td>\n",
       "      <td>-0.220385</td>\n",
       "      <td>0.022298</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0    0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1    0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2    1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3    1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4    2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "5    2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "6    4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7    7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8    7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "9    9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "10  10.0  1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152   \n",
       "11  10.0  0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027   \n",
       "12  10.0  1.249999 -1.221637  0.383930 -1.234899 -1.485419 -0.753230   \n",
       "13  11.0  1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544   \n",
       "14  12.0 -2.791855 -0.327771  1.641750  1.767473 -0.136588  0.807596   \n",
       "15  12.0 -0.752417  0.345485  2.057323 -1.468643 -1.158394 -0.077850   \n",
       "16  12.0  1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069   \n",
       "17  13.0 -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867   \n",
       "18  14.0 -5.401258 -5.450148  1.186305  1.736239  3.049106 -1.763406   \n",
       "19  15.0  1.492936 -1.029346  0.454795 -1.438026 -1.555434 -0.720961   \n",
       "\n",
       "          V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0   0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2   0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3   0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4   0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "5   0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427   \n",
       "6  -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055   \n",
       "7   1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709   \n",
       "8   0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592   \n",
       "9   0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050   \n",
       "10 -1.423236  0.048456 -1.720408  ... -0.009302  0.313894  0.027740  0.500512   \n",
       "11  0.470455  0.538247 -0.558895  ...  0.049924  0.238422  0.009130  0.996710   \n",
       "12 -0.689405 -0.227487 -2.094011  ... -0.231809 -0.483285  0.084668  0.392831   \n",
       "13 -0.096717  0.115982 -0.221083  ... -0.036876  0.074412 -0.071407  0.104744   \n",
       "14 -0.422911 -1.907107  0.755713  ...  1.151663  0.222182  1.020586  0.028317   \n",
       "15 -0.608581  0.003603 -0.436167  ...  0.499625  1.353650 -0.256573 -0.065084   \n",
       "16 -0.586057  0.189380  0.782333  ... -0.024612  0.196002  0.013802  0.103758   \n",
       "17  0.707642  0.087962 -0.665271  ... -0.194796 -0.672638 -0.156858 -0.888386   \n",
       "18 -1.559738  0.160842  1.233090  ... -0.503600  0.984460  2.458589  0.042119   \n",
       "19 -1.080664 -0.053127 -1.978682  ... -0.177650 -0.175074  0.040002  0.295814   \n",
       "\n",
       "         V25       V26       V27       V28  Amount  Class  \n",
       "0   0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1   0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2  -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3   0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "5  -0.232794  0.105915  0.253844  0.081080    3.67      0  \n",
       "6   0.750137 -0.257237  0.034507  0.005168    4.99      0  \n",
       "7  -0.415267 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "8   0.373205 -0.384157  0.011747  0.142404   93.20      0  \n",
       "9  -0.069733  0.094199  0.246219  0.083076    3.68      0  \n",
       "10  0.251367 -0.129478  0.042850  0.016253    7.80      0  \n",
       "11 -0.767315 -0.492208  0.042472 -0.054337    9.99      0  \n",
       "12  0.161135 -0.354990  0.026416  0.042422  121.50      0  \n",
       "13  0.548265  0.104094  0.021491  0.021293   27.50      0  \n",
       "14 -0.232746 -0.235557 -0.164778 -0.030154   58.80      0  \n",
       "15 -0.039124 -0.087086 -0.180998  0.129394   15.99      0  \n",
       "16  0.364298 -0.382261  0.092809  0.037051   12.99      0  \n",
       "17 -0.342413 -0.049027  0.079692  0.131024    0.89      0  \n",
       "18 -0.481631 -0.621272  0.392053  0.949594   46.80      0  \n",
       "19  0.332931 -0.220385  0.022298  0.007602    5.00      0  \n",
       "\n",
       "[20 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(PATH + 'creditcard.csv')\n",
    "\n",
    "data.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graphs(df):\n",
    "    class_counts = df['Class'].value_counts()\n",
    "    ax = class_counts.plot(kind='bar')\n",
    "    for idx, value in class_counts.iteritems():\n",
    "        ax.annotate(value,\n",
    "                    (idx, value),\n",
    "                    xytext=(0, 5),\n",
    "                    horizontalalignment='center',\n",
    "                    textcoords='offset points')\n",
    "    plt.title('Fraud occurrences')\n",
    "    plt.xlabel('Fraud')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(data['Amount'])\n",
    "    plt.title('Histogram of \\'Amount\\'')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(np.log10(.1 + data['Amount']))\n",
    "    plt.title('Histogram of log(1+\\'Amount\\')')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAETCAYAAAD6R0vDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdSUlEQVR4nO3dfZxVZb338c9XkNBU5El8AAWLUhRDGZXbTk92VDA9kpLHzimnwjimnltKX2adOpplRd1Fekrvg0kiaqiYxq2Y4sOxtKMyJqFgyhwdYZAQedTwaeB3/7GuwcU4DBtde29mz/f9eu3XrP1b61rrWjrwZV3r2msrIjAzMyvSDtXugJmZ1R6Hi5mZFc7hYmZmhXO4mJlZ4RwuZmZWOIeLmZkVzuFi9i5JukbS96rdD7PticPFaoakJkmvSnol99q72v0y64ocLlZrToyIXXKvF/IrJXWvVseqoe35KuM/91Z2/iWzmicpJJ0taRGwKNUuk7RE0jpJj0n6SG77zYa5JH1cUnPu/aGS/iTpZUk3Aj07OPYOkr4l6XlJL0q6VlKv3Pq/k/RHSWtSf76Q6jtJ+klqt1bSg6m2WV/Stk2S/j4tXyxppqTrJK0DviDpvyRdKukhYD2wv6QDJM2RtErS05JObXP+v5B0RzrHRyS9L7f+oFzb5ZK+mTvXCyX9j6SVkm6S1Cet65n6tDKd61xJA7bt/6R1Jg4X6yrGAkcCw9L7ucAIoA9wA3CzpC2GRCtJPYDbgOmp7c3AKR00+UJ6fQLYH9gF+Hna137AncB/AP1Tf+aldv8HGAkclY5zAbBxa/1LTgJmArsD16fa54EJwK7ACmAO2XnvAZwGXCFpWG4fpwHfAXoDjcClqc+7AvcAvwP2Bt4P3Jva/CvZf+ePpXWrgV+kdfVAL2AQ0Bc4E3i1xPOxTsjhYrXmtvQv4zWSbsvVfxARqyLiVYCIuC4iVkZES0T8BHgP8MES9j8K2BH4WUS8GREzyYJqS/4Z+GlEPBsRrwDfAE5Lw1X/BNwTEb9O+1oZEfPSsNWXgHMjYmlEbIiIP0bE6yX+N/jviLgtIja2ni9wTUQsiIgWYDTQFBG/Suf/OHAL8JncPm6NiEfT9teTBR/ACcBfI+InEfFaRLwcEY+kdWcC/xYRzamvFwPj0rm+SRYq70/n81hErCvxfKwT6lLjz9YljI2Ie9qpL8m/kXQ+MJ7sX9gB7Ab0K2H/ewNLY/Mnvj6/le3z658n+3M3gOxf8f/TTpt+ZENt7a0rxZKt1PYDjpS0JlfrTnY11uqvueX1ZFdcsOU+t+73Vkn5K6wNZOc6PbWdIWl34DqyIHqzwzOxTstXLtZVbAqDdH/lAuBUoHdE7A6sBZQ2+Ruwc67tnrnlZcA+kpSr7dvBcV8g+0s3v20LsJzsL/z3tdPmJeC1LazbrG+SupENqeW196jzfG0J8EBE7J577RIRX+ngPPJt9+9g3Zg2++2Zrr7ejIjvRMQwsqG+E4DTSziedVIOF+uKdiX7C34F0F3Sv5NdubSaBxwvqY+kPYGJuXX/ndr+b0k7SjoZOKKDY/0a+KqkIZJ2Ab4P3Jgbbvp7SadK6i6pr6QREbERmAr8VNLekrpJ+l+S3gM8A/SU9ClJOwLfIhvS2xa3Ax+Q9Pl0DjtKOlzSgSW23UvSREnvkbSrpCPTuv8LXJruJSGpv6ST0vInJA1PYbiObJis1HtI1gk5XKwruovshvQzZMNUr7H5sNF04M9AE3A3cGPrioh4AziZ7Cb9KuAfgd90cKypaX+/B55Lx/rXtK/FwPHAeWlf84APpXbnA0+Q3c9ZBUwCdoiItcBZwC+BpWRXMpvNHtuaiHgZOJbspv0LZENgkyghpFLbY4ATU7tFZJMVAC4DZgF3S3oZeJhsEgVkV38zyYLlKeABNh+Gsxojf1mYmZkVzVcuZmZWOIeLmZkVzuFiZmaFc7jYJpIGSbpf0kJJCySdm+ojJD0saZ6kBklHtGl3uKQWSePS+/2UPR5lXtrPmbltL02POXmlzT6+IGlFajNP0hmVOGczKw/f0LdNJO0F7BURf0qP+XiM7HEePwMmR8Sdko4HLoiIj6c23cgeJfIaMDUiZqZHpCgiXk/Tb58EjoqIFySNIpuhtSgidskd+wtAXUScU6HTNbMycrgk/fr1i8GDB1e7G9uVxsZG9thjD5YvX07fvn3p06cPq1atYs2aNey/f/Y5uuXLlyOJ9evX06tXL3r37r3ZPlpaWli4cCEHHHAAPXr02FR//PHHOfTQQze9f+mll1i/fj377tvR5xHNbHvz2GOPvRQRbT/ICxHhVwQjR44Me8tzzz0XgwYNirVr18bChQtj0KBBMXDgwNh7772jqakpIiKam5vjox/9aGzYsCHq6+vj5ptv3tR+8eLFMXz48Nhpp53i5z//+dv2/973vnez97/61a9izz33jOHDh8cpp5wSixcvLu8JmlkhgIZo5+9U33Oxt3nllVc45ZRT+NnPfsZuu+3GlVdeyeTJk1myZAmTJ09m/PjxAEycOJFJkyaxww5v/zUaNGgQ8+fPp7GxkWnTprF8+fIOj3niiSfS1NTE/PnzOeaYY6ivry/LuZlZZXhYLKmrq4uGhoZqd6Pq3nzzTU444QSOO+44vva1rwHQq1cv1qxZgyQigl69erFu3TqGDBlC6+/PSy+9xM4778yUKVMYO3bsZvv80pe+xPHHH8+4ceM21XbZZRdeeWWze/qbbNiwgT59+rB27drynKSZFUbSYxFR17buKxfbJCIYP348Bx544KZgAdh777154IEHALjvvvsYOnQoAM899xxNTU00NTUxbtw4rrjiCsaOHUtzczOvvpo96X316tU8+OCDfPCDHT/NftmyZZuWZ82axYEHlvKYKzPbXvmR+7bJQw89xPTp0xk+fDgjRowA4Pvf/z5XXXUV5557Li0tLfTs2ZMpU6Z0uJ+nnnqK8847b9OVzvnnn8/w4cMBuOCCC7jhhhtYv349AwcO5IwzzuDiiy/m8ssvZ9asWXTv3p0+ffpwzTXXlPlszaycPCyWeFjMzGzbeVjMzMwqxuFiZmaFc7iYmVnhfEO/kxl84R3V7kJNafrhp6rdBbOa5CsXMzMrnMPFzMwK53AxM7PCOVzMzKxwDhczMyucw8XMzArncDEzs8I5XMzMrHAOFzMzK5zDxczMCudwMTOzwjlczMyscA4XMzMrnMPFzMwK53AxM7PCOVzMzKxwDhczMyucw8XMzArncDEzs8I5XMzMrHAOFzMzK1zZwkXSIEn3S1ooaYGkc1P9YklLJc1Lr+Nzbb4hqVHS05KOy9VHp1qjpAtz9SGSHkn1GyX1SPX3pPeNaf3gcp2nmZm9XTmvXFqA8yJiGDAKOFvSsLRuckSMSK/ZAGndacBBwGjgCkndJHUDfgGMAYYBn83tZ1La1/uB1cD4VB8PrE71yWk7MzOrkLKFS0Qsi4g/peWXgaeAfTpochIwIyJej4jngEbgiPRqjIhnI+INYAZwkiQBRwMzU/tpwNjcvqal5ZnAJ9P2ZmZWARW555KGpQ4FHkmlcyTNlzRVUu9U2wdYkmvWnGpbqvcF1kRES5v6ZvtK69em7dv2a4KkBkkNK1aseHcnaWZmm5Q9XCTtAtwCTIyIdcCVwPuAEcAy4Cfl7sOWRMSUiKiLiLr+/ftXqxtmZjWnrOEiaUeyYLk+In4DEBHLI2JDRGwEriIb9gJYCgzKNR+YaluqrwR2l9S9TX2zfaX1vdL2ZmZWAeWcLSbgauCpiPhprr5XbrNPA0+m5VnAaWmm1xBgKPAoMBcYmmaG9SC76T8rIgK4HxiX2tcDv83tqz4tjwPuS9ubmVkFdN/6Ju/Yh4HPA09Impdq3ySb7TUCCKAJ+BeAiFgg6SZgIdlMs7MjYgOApHOAu4BuwNSIWJD293VghqTvAY+ThRnp53RJjcAqskAyM7MKKVu4RMSDQHsztGZ30OZS4NJ26rPbaxcRz/LWsFq+/hrwmW3pr5mZFcef0Dczs8I5XMzMrHAOFzMzK5zDxczMCudwMTOzwjlczMyscA4XMzMrnMPFzMwK53AxM7PCOVzMzKxwDhczMyucw8XMzArncDEzs8I5XMzMrHAOFzMzK5zDxczMCudwMTOzwjlczMyscA4XMzMrnMPFzMwK53AxM7PCOVzMzKxwDhczMyucw8XMzArncDEzs8I5XMzMrHAOFzMzK1zZwkXSIEn3S1ooaYGkc1O9j6Q5khaln71TXZIul9Qoab6kw3L7qk/bL5JUn6uPlPREanO5JHV0DDMzq4xyXrm0AOdFxDBgFHC2pGHAhcC9ETEUuDe9BxgDDE2vCcCVkAUFcBFwJHAEcFEuLK4EvpxrNzrVt3QMMzOrgLKFS0Qsi4g/peWXgaeAfYCTgGlps2nA2LR8EnBtZB4Gdpe0F3AcMCciVkXEamAOMDqt2y0iHo6IAK5ts6/2jmFmZhVQkXsukgYDhwKPAAMiYlla9VdgQFreB1iSa9acah3Vm9up08ExzMysAsoeLpJ2AW4BJkbEuvy6dMUR5Tx+R8eQNEFSg6SGFStWlLMbZmZdSlnDRdKOZMFyfUT8JpWXpyEt0s8XU30pMCjXfGCqdVQf2E69o2NsJiKmRERdRNT179//nZ2kmZm9TTlniwm4GngqIn6aWzULaJ3xVQ/8Nlc/Pc0aGwWsTUNbdwHHSuqdbuQfC9yV1q2TNCod6/Q2+2rvGGZmVgHdy7jvDwOfB56QNC/Vvgn8ELhJ0njgeeDUtG42cDzQCKwHvggQEaskfReYm7a7JCJWpeWzgGuAnYA704sOjmFmZhVQtnCJiAcBbWH1J9vZPoCzt7CvqcDUduoNwMHt1Fe2dwwzM6sMf0LfzMwK53AxM7PCOVzMzKxwDhczMyucw8XMzArncDEzs8I5XMzMrHAOFzMzK1xJ4SJpeLk7YmZmtaPUK5crJD0q6SxJvcraIzMz6/RKCpeI+Ajwz2RPJ35M0g2Sjilrz8zMrNMq+Z5LRCwCvgV8HfgYcLmkv0g6uVydMzOzzqnUey6HSJpM9lXFRwMnRsSBaXlyGftnZmadUKlPRf4P4JfANyPi1dZiRLwg6Vtl6ZmZmXVapYbLp4BXI2IDgKQdgJ4RsT4ippetd2Zm1imVes/lHrIv5Gq1c6qZmZm9Tanh0jMiXml9k5Z3Lk+XzMyssys1XP4m6bDWN5JGAq92sL2ZmXVhpd5zmQjcLOkFsq8u3hP4x3J1yszMOreSwiUi5ko6APhgKj0dEW+Wr1tmZtaZlXrlAnA4MDi1OUwSEXFtWXplZmadWknhImk68D5gHrAhlQNwuJiZ2duUeuVSBwyLiChnZ8zMrDaUOlvsSbKb+GZmZltV6pVLP2ChpEeB11uLEfEPZemVmZl1aqWGy8Xl7ISZmdWWUqciPyBpP2BoRNwjaWegW3m7ZmZmnVWpj9z/MjAT+M9U2ge4bSttpkp6UdKTudrFkpZKmpdex+fWfUNSo6SnJR2Xq49OtUZJF+bqQyQ9kuo3SuqR6u9J7xvT+sGlnKOZmRWn1Bv6ZwMfBtbBpi8O22Mrba4BRrdTnxwRI9JrNoCkYcBpwEGpzRWSuknqBvwCGAMMAz6btgWYlPb1fmA1MD7VxwOrU31y2s7MzCqo1HB5PSLeaH0jqTvZ51y2KCJ+D6wqcf8nATMi4vWIeA5oBI5Ir8aIeDYdfwZwkiSRfVHZzNR+GjA2t69paXkm8Mm0vZmZVUip4fKApG8CO0k6BrgZ+H/v8JjnSJqfhs16p9o+wJLcNs2ptqV6X2BNRLS0qW+2r7R+bdrezMwqpNRwuRBYATwB/AswG3gn30B5Jdkn/UcAy4CfvIN9FEbSBEkNkhpWrFhRza6YmdWUUmeLbQSuSq93LCKWty5Lugq4Pb1dCgzKbTow1dhCfSWwu6Tu6eokv33rvprT8F2vtH17/ZkCTAGoq6vz0wfMzApS6myx5yQ92/a1rQeTtFfu7afJPvkPMAs4Lc30GgIMBR4F5gJD08ywHmQ3/Welx9DcD4xL7euB3+b2VZ+WxwH3+bE1ZmaVtS3PFmvVE/gM0KejBpJ+DXwc6CepGbgI+LikEWSTAZrIhtiIiAWSbgIWAi3A2RGxIe3nHOAuss/VTI2IBekQXwdmSPoe8DhwdapfDUyX1Eg2oeC0Es/RzMwKonf6j3pJj0XEyIL7UzV1dXXR0NBQ7W5s1eAL76h2F2pK0w8/Ve0umHVqKQvq2tZLfeT+Ybm3O5BdyWzLd8GYmVkXUmpA5Gd1tZANaZ1aeG/MzKwmlDpb7BPl7oiZmdWOUofFvtbR+oj4aTHdMTOzWrAts8UOJ5vmC3Ai2VThReXolJmZdW6lhstA4LCIeBmypxsDd0TE58rVMTMz67xKffzLAOCN3Ps3Us3MzOxtSr1yuRZ4VNKt6f1Y3nrysJmZ2WZKnS12qaQ7gY+k0hcj4vHydcvMzDqzUofFAHYG1kXEZWQPhRxSpj6ZmVknV+qDKy8ie5bXN1JpR+C6cnXKzMw6t1KvXD4N/APwN4CIeAHYtVydMjOzzq3UcHkjPbY+ACS9t3xdMjOzzq7UcLlJ0n+SfUHXl4F7eJdfHGZmZrVrq7PFJAm4ETgAWAd8EPj3iJhT5r6ZmVkntdVwiYiQNDsihgMOFDMz26pSh8X+JOnwsvbEzMxqRqmf0D8S+JykJrIZYyK7qDmkXB0zM7POq8NwkbRvRCwGjqtQf8zMrAZs7crlNrKnIT8v6ZaIOKUCfTIzs05ua/dclFvev5wdMTOz2rG1cIktLJuZmW3R1obFPiRpHdkVzE5pGd66ob9bWXtnZmadUofhEhHdKtURMzOrHdvyyH0zM7OSOFzMzKxwDhczMyucw8XMzApXtnCRNFXSi5KezNX6SJojaVH62TvVJelySY2S5ks6LNemPm2/SFJ9rj5S0hOpzeXp6c1bPIaZmVVOOa9crgFGt6ldCNwbEUOBe9N7gDHA0PSaAFwJWVAAF5E92+wI4KJcWFwJfDnXbvRWjmFmZhVStnCJiN8Dq9qUTwKmpeVpwNhc/drIPEz2pWR7kT3TbE5ErIqI1WSP/B+d1u0WEQ+nb8i8ts2+2juGmZlVSKXvuQyIiGVp+a/AgLS8D7Akt11zqnVUb26n3tExzMysQqp2Qz9dcZT1kTJbO4akCZIaJDWsWLGinF0xM+tSKh0uy9OQFunni6m+FBiU225gqnVUH9hOvaNjvE1ETImIuoio69+//zs+KTMz21ylw2UW0Drjqx74ba5+epo1NgpYm4a27gKOldQ73cg/FrgrrVsnaVSaJXZ6m321dwwzM6uQUr+JcptJ+jXwcaCfpGayWV8/BG6SNB54Hjg1bT4bOB5oBNYDXwSIiFWSvgvMTdtdEhGtkwTOIpuRthNwZ3rRwTHMzKxCyhYuEfHZLaz6ZDvbBnD2FvYzFZjaTr0BOLid+sr2jmFmZpXjT+ibmVnhHC5mZlY4h4uZmRXO4WJmZoVzuJiZWeEcLmZmVjiHi5mZFc7hYmZmhXO4mJlZ4RwuZmZWOIeLmZkVzuFiZmaFc7iYmVnhHC5mZlY4h4uZmRXO4WJmZoVzuJiZWeEcLmZmVjiHi5mZFc7hYmZmhXO4mJlZ4RwuZmZWOIeLmZkVzuFiZmaFc7iYmVnhHC5mZlY4h4uZmRWuKuEiqUnSE5LmSWpItT6S5khalH72TnVJulxSo6T5kg7L7ac+bb9IUn2uPjLtvzG1VeXP0sys66rmlcsnImJERNSl9xcC90bEUODe9B5gDDA0vSYAV0IWRsBFwJHAEcBFrYGUtvlyrt3o8p+OmZm12p6GxU4CpqXlacDYXP3ayDwM7C5pL+A4YE5ErIqI1cAcYHRat1tEPBwRAVyb25eZmVVAtcIlgLslPSZpQqoNiIhlafmvwIC0vA+wJNe2OdU6qje3UzczswrpXqXj/l1ELJW0BzBH0l/yKyMiJEW5O5GCbQLAvvvuW+7DmZl1GVW5comIpenni8CtZPdMlqchLdLPF9PmS4FBueYDU62j+sB26u31Y0pE1EVEXf/+/d/taZmZWVLxcJH0Xkm7ti4DxwJPArOA1hlf9cBv0/Is4PQ0a2wUsDYNn90FHCupd7qRfyxwV1q3TtKoNEvs9Ny+zMysAqoxLDYAuDXNDu4O3BARv5M0F7hJ0njgeeDUtP1s4HigEVgPfBEgIlZJ+i4wN213SUSsSstnAdcAOwF3ppeZmVVIxcMlIp4FPtROfSXwyXbqAZy9hX1NBaa2U28ADn7XnTUzs3dke5qKbGZmNcLhYmZmhXO4mJlZ4RwuZmZWOIeLmZkVzuFiZmaFc7iYmVnhHC5mZlY4h4uZmRXO4WJmZoVzuJiZWeEcLmZmVjiHi5mZFc7hYmZmhXO4mJlZ4RwuZmZWOIeLmZkVzuFiZmaFc7iYWc3ZsGEDhx56KCeccAIA9913H4cddhgHH3ww9fX1tLS0AHD99ddzyCGHMHz4cI466ij+/Oc/V7PbNcXhYmY157LLLuPAAw8EYOPGjdTX1zNjxgyefPJJ9ttvP6ZNmwbAkCFDeOCBB3jiiSf49re/zYQJE6rZ7ZricDGzmtLc3Mwdd9zBGWecAcDKlSvp0aMHH/jABwA45phjuOWWWwA46qij6N27NwCjRo2iubm5Op2uQQ4XM6spEydO5Ec/+hE77JD99davXz9aWlpoaGgAYObMmSxZsuRt7a6++mrGjBlT0b7WMoeLmdWM22+/nT322IORI0duqklixowZfPWrX+WII45g1113pVu3bpu1u//++7n66quZNGlSpbtcs7pXuwNmZkV56KGHmDVrFrNnz+a1115j3bp1fO5zn+O6667jD3/4AwB33303zzzzzKY28+fP54wzzuDOO++kb9++1ep6zfGVi5nVjB/84Ac0NzfT1NTEjBkzOProo7nuuut48cUXAXj99deZNGkSZ555JgCLFy/m5JNPZvr06ZvuyVgxfOViZjXvxz/+MbfffjsbN27kK1/5CkcffTQAl1xyCStXruSss84CoHv37pvuzdi7o4iodh+2C3V1ddEZfqkGX3hHtbtQU5p++Klqd8GsU5P0WETUta17WMzMzApXs+EiabSkpyU1Srqw2v0xM+tKajJcJHUDfgGMAYYBn5U0rLq9MjPrOmoyXIAjgMaIeDYi3gBmACdVuU9mZl1Grc4W2wfIfwS3GTiy7UaSJgCtDxN6RdLTFehbV9EPeKnandga+TNzXVGn+N3sRPZrr1ir4VKSiJgCTKl2P2qRpIb2ZpCYVZt/NyujVofFlgKDcu8HppqZmVVArYbLXGCopCGSegCnAbOq3Cczsy6jJofFIqJF0jnAXUA3YGpELKhyt7oaDzfa9sq/mxXgT+ibmVnhanVYzMzMqsjhYmZmhXO4mJlZ4Wryhr5VlqQDyJ6AsE8qLQVmRcRT1euVmVWTr1zsXZH0dbLH6wh4NL0E/NoPDLXtmaQvVrsPtcyzxexdkfQMcFBEvNmm3gNYEBFDq9Mzs45JWhwR+1a7H7XKw2L2bm0E9gaeb1PfK60zqxpJ87e0ChhQyb50NQ4Xe7cmAvdKWsRbDwvdF3g/cE61OmWWDACOA1a3qQv4Y+W703U4XOxdiYjfSfoA2dcc5G/oz42IDdXrmRkAtwO7RMS8tisk/VfFe9OF+J6LmZkVzrPFzMyscA4XMzMrnMPFrIIkbZA0L/caXIZjNEnqV/R+zbaFb+ibVdarETGivRWSRHYf1FO4rdPzlYtZFUkaLOlpSdcCTwKDJF0pqUHSAknfyW276YpEUl3rbCdJfSXdnbb/Jdk0W7OqcriYVdZOuSGxW1NtKHBFRBwUEc8D/5a+4/0Q4GOSDtnKPi8CHoyIg4BbyT5nZFZVHhYzq6zNhsXSPZfnI+Lh3DanSppA9udzL2AYsKVPmgN8FDgZICLukNT2A4NmFedwMau+v7UuSBoCnA8cHhGrJV0D9EyrW3hrtKEnZtsxD4uZbV92IwubtZIGAGNy65qAkWn5lFz998A/AUgaA/QufzfNOuZwMduORMSfgceBvwA3AA/lVn8HuExSA7ChTf2jkhaQDY8trlB3zbbIj38xM7PC+crFzMwK53AxM7PCOVzMzKxwDhczMyucw8XMzArncDEzs8I5XMzMrHAOFzMzK9z/B8ef1f7Bvd9OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graphs(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Split Alice & Bob datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split_custom(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    279395\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only clean data in the train set (to train the autoencoder)\n",
    "data_train['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4920\n",
       "1     492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1/10 of the test set is fraud data\n",
    "data_test['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alice, data_bob = zip(np.array_split(data_train, 2),\n",
    "                           np.array_split(data_test, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice train set : (139698, 31)\n",
      "Alice test set : (2706, 31)\n",
      "\n",
      "Bob train set : (139697, 31)\n",
      "Bob test set : (2706, 31)\n"
     ]
    }
   ],
   "source": [
    "print(f'Alice train set : {data_alice[0].shape}')\n",
    "print(f'Alice test set : {data_alice[1].shape}')\n",
    "\n",
    "print(f'\\nBob train set : {data_bob[0].shape}')\n",
    "print(f'Bob test set : {data_bob[1].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Dense(input_dim, activation='tanh'),\n",
    "            layers.Dense(20, activation='tanh'),\n",
    "            layers.Dense(15, activation='tanh'),\n",
    "            layers.Dense(latent_dim, activation='elu')\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(20, activation='tanh'),\n",
    "            layers.Dense(input_dim, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Train the classifier on the encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7768 - val_loss: 0.6346\n",
      "Epoch 2/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.6702 - val_loss: 0.5972\n",
      "Epoch 3/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.6390 - val_loss: 0.5696\n",
      "Epoch 4/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.6161 - val_loss: 0.5531\n",
      "Epoch 5/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.6018 - val_loss: 0.5411\n",
      "Epoch 6/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5910 - val_loss: 0.5312\n",
      "Epoch 7/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5818 - val_loss: 0.5237\n",
      "Epoch 8/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5747 - val_loss: 0.5178\n",
      "Epoch 9/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5690 - val_loss: 0.5121\n",
      "Epoch 10/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5641 - val_loss: 0.5086\n",
      "Epoch 11/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5604 - val_loss: 0.5051\n",
      "Epoch 12/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5572 - val_loss: 0.5023\n",
      "Epoch 13/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5545 - val_loss: 0.4997\n",
      "Epoch 14/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5520 - val_loss: 0.4976\n",
      "Epoch 15/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5499 - val_loss: 0.4956\n",
      "Epoch 16/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5478 - val_loss: 0.4934\n",
      "Epoch 17/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5460 - val_loss: 0.4916\n",
      "Epoch 18/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5443 - val_loss: 0.4902\n",
      "Epoch 19/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5426 - val_loss: 0.4883\n",
      "Epoch 20/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5412 - val_loss: 0.4871\n",
      "Epoch 21/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5397 - val_loss: 0.4858\n",
      "Epoch 22/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5384 - val_loss: 0.4844\n",
      "Epoch 23/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5371 - val_loss: 0.4833\n",
      "Epoch 24/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5359 - val_loss: 0.4818\n",
      "Epoch 25/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5347 - val_loss: 0.4810\n",
      "Epoch 26/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5334 - val_loss: 0.4797\n",
      "Epoch 27/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5324 - val_loss: 0.4783\n",
      "Epoch 28/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5313 - val_loss: 0.4777\n",
      "Epoch 29/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5303 - val_loss: 0.4764\n",
      "Epoch 30/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5292 - val_loss: 0.4752\n",
      "Epoch 31/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5281 - val_loss: 0.4744\n",
      "Epoch 32/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5270 - val_loss: 0.4735\n",
      "Epoch 33/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5261 - val_loss: 0.4723\n",
      "Epoch 34/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5250 - val_loss: 0.4712\n",
      "Epoch 35/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5239 - val_loss: 0.4702\n",
      "Epoch 36/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5229 - val_loss: 0.4689\n",
      "Epoch 37/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5221 - val_loss: 0.4683\n",
      "Epoch 38/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5209 - val_loss: 0.4670\n",
      "Epoch 39/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5200 - val_loss: 0.4660\n",
      "Epoch 40/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5192 - val_loss: 0.4653\n",
      "Epoch 41/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5183 - val_loss: 0.4645\n",
      "Epoch 42/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5176 - val_loss: 0.4642\n",
      "Epoch 43/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5168 - val_loss: 0.4630\n",
      "Epoch 44/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5161 - val_loss: 0.4622\n",
      "Epoch 45/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5155 - val_loss: 0.4619\n",
      "Epoch 46/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5147 - val_loss: 0.4608\n",
      "Epoch 47/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5140 - val_loss: 0.4607\n",
      "Epoch 48/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5134 - val_loss: 0.4595\n",
      "Epoch 49/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5128 - val_loss: 0.4588\n",
      "Epoch 50/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.5122 - val_loss: 0.4583\n",
      "Epoch 51/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5116 - val_loss: 0.4577\n",
      "Epoch 52/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5110 - val_loss: 0.4569\n",
      "Epoch 53/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5104 - val_loss: 0.4562\n",
      "Epoch 54/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5099 - val_loss: 0.4555\n",
      "Epoch 55/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5093 - val_loss: 0.4546\n",
      "Epoch 56/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5088 - val_loss: 0.4549\n",
      "Epoch 57/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5081 - val_loss: 0.4537\n",
      "Epoch 58/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5075 - val_loss: 0.4534\n",
      "Epoch 59/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5069 - val_loss: 0.4528\n",
      "Epoch 60/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5064 - val_loss: 0.4526\n",
      "Epoch 61/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5059 - val_loss: 0.4514\n",
      "Epoch 62/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5054 - val_loss: 0.4511\n",
      "Epoch 63/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5050 - val_loss: 0.4508\n",
      "Epoch 64/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5043 - val_loss: 0.4500\n",
      "Epoch 65/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5041 - val_loss: 0.4501\n",
      "Epoch 66/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5036 - val_loss: 0.4493\n",
      "Epoch 67/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5033 - val_loss: 0.4488\n",
      "Epoch 68/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5027 - val_loss: 0.4486\n",
      "Epoch 69/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5023 - val_loss: 0.4480\n",
      "Epoch 70/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5021 - val_loss: 0.4474\n",
      "Epoch 71/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5018 - val_loss: 0.4476\n",
      "Epoch 72/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5015 - val_loss: 0.4477\n",
      "Epoch 73/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5010 - val_loss: 0.4466\n",
      "Epoch 74/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5006 - val_loss: 0.4464\n",
      "Epoch 75/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5007 - val_loss: 0.4464\n",
      "Epoch 76/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5005 - val_loss: 0.4457\n",
      "Epoch 77/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4999 - val_loss: 0.4455\n",
      "Epoch 78/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4997 - val_loss: 0.4452\n",
      "Epoch 79/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4993 - val_loss: 0.4450\n",
      "Epoch 80/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4990 - val_loss: 0.4446\n",
      "Epoch 81/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4988 - val_loss: 0.4442\n",
      "Epoch 82/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4985 - val_loss: 0.4441\n",
      "Epoch 83/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4984 - val_loss: 0.4438\n",
      "Epoch 84/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4982 - val_loss: 0.4442\n",
      "Epoch 85/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4979 - val_loss: 0.4431\n",
      "Epoch 86/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4976 - val_loss: 0.4437\n",
      "Epoch 87/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4977 - val_loss: 0.4439\n",
      "Epoch 88/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4973 - val_loss: 0.4427\n",
      "Epoch 89/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4971 - val_loss: 0.4428\n",
      "Epoch 90/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4971 - val_loss: 0.4423\n",
      "Epoch 91/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4968 - val_loss: 0.4432\n",
      "Epoch 92/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4967 - val_loss: 0.4424\n",
      "Epoch 93/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4965 - val_loss: 0.4421\n",
      "Epoch 94/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4962 - val_loss: 0.4418\n",
      "Epoch 95/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4960 - val_loss: 0.4425\n",
      "Epoch 96/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4959 - val_loss: 0.4420\n",
      "Epoch 97/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4958 - val_loss: 0.4418\n",
      "Epoch 98/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4958 - val_loss: 0.4409\n",
      "Epoch 99/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4954 - val_loss: 0.4410\n",
      "Epoch 100/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4952 - val_loss: 0.4413\n",
      "Epoch 101/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4951 - val_loss: 0.4407\n",
      "Epoch 102/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4950 - val_loss: 0.4409\n",
      "Epoch 103/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4948 - val_loss: 0.4406\n",
      "Epoch 104/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4947 - val_loss: 0.4405\n",
      "Epoch 105/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4946 - val_loss: 0.4407\n",
      "Epoch 106/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4946 - val_loss: 0.4409\n",
      "Epoch 107/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4943 - val_loss: 0.4401\n",
      "Epoch 108/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4944 - val_loss: 0.4401\n",
      "Epoch 109/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4940 - val_loss: 0.4398\n",
      "Epoch 110/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4939 - val_loss: 0.4398\n",
      "Epoch 111/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4941 - val_loss: 0.4397\n",
      "Epoch 112/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4937 - val_loss: 0.4398\n",
      "Epoch 113/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4938 - val_loss: 0.4393\n",
      "Epoch 114/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4937 - val_loss: 0.4395\n",
      "Epoch 115/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4933 - val_loss: 0.4395\n",
      "Epoch 116/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4933 - val_loss: 0.4391\n",
      "Epoch 117/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4934 - val_loss: 0.4395\n",
      "Epoch 118/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4932 - val_loss: 0.4389\n",
      "Epoch 119/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4928 - val_loss: 0.4389\n",
      "Epoch 120/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4928 - val_loss: 0.4384\n",
      "Epoch 121/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4927 - val_loss: 0.4386\n",
      "Epoch 122/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4926 - val_loss: 0.4385\n",
      "Epoch 123/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4927 - val_loss: 0.4383\n",
      "Epoch 124/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4923 - val_loss: 0.4381\n",
      "Epoch 125/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4923 - val_loss: 0.4384\n",
      "Epoch 126/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4922 - val_loss: 0.4384\n",
      "Epoch 127/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4923 - val_loss: 0.4382\n",
      "Epoch 128/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4921 - val_loss: 0.4385\n",
      "Epoch 129/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4922 - val_loss: 0.4390\n",
      "Epoch 130/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4922 - val_loss: 0.4389\n",
      "Epoch 131/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4918 - val_loss: 0.4377\n",
      "Epoch 132/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4915 - val_loss: 0.4376\n",
      "Epoch 133/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4918 - val_loss: 0.4371\n",
      "Epoch 134/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4915 - val_loss: 0.4376\n",
      "Epoch 135/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4915 - val_loss: 0.4374\n",
      "Epoch 136/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4913 - val_loss: 0.4370\n",
      "Epoch 137/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4913 - val_loss: 0.4372\n",
      "Epoch 138/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4910 - val_loss: 0.4371\n",
      "Epoch 139/250\n",
      "492/492 [==============================] - 1s 3ms/step - loss: 0.4910 - val_loss: 0.4372\n",
      "Epoch 140/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4910 - val_loss: 0.4366\n",
      "Epoch 141/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4908 - val_loss: 0.4367\n",
      "Epoch 142/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4907 - val_loss: 0.4370\n",
      "Epoch 143/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4907 - val_loss: 0.4370\n",
      "Epoch 144/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4905 - val_loss: 0.4360\n",
      "Epoch 145/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4906 - val_loss: 0.4372\n",
      "Epoch 146/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4905 - val_loss: 0.4370\n",
      "Epoch 147/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4903 - val_loss: 0.4359\n",
      "Epoch 148/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4903 - val_loss: 0.4361\n",
      "Epoch 149/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4903 - val_loss: 0.4361\n",
      "Epoch 150/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4901 - val_loss: 0.4361\n",
      "Epoch 151/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4902 - val_loss: 0.4354\n",
      "Epoch 152/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4900 - val_loss: 0.4355\n",
      "Epoch 153/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4899 - val_loss: 0.4361\n",
      "Epoch 154/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4898 - val_loss: 0.4355\n",
      "Epoch 155/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4900 - val_loss: 0.4363\n",
      "Epoch 156/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4897 - val_loss: 0.4354\n",
      "Epoch 157/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4897 - val_loss: 0.4353\n",
      "Epoch 158/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4896 - val_loss: 0.4357\n",
      "Epoch 159/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4896 - val_loss: 0.4351\n",
      "Epoch 160/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4894 - val_loss: 0.4353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4893 - val_loss: 0.4372\n",
      "Epoch 162/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4894 - val_loss: 0.4357\n",
      "Epoch 163/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4892 - val_loss: 0.4350\n",
      "Epoch 164/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4892 - val_loss: 0.4349\n",
      "Epoch 165/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4891 - val_loss: 0.4354\n",
      "Epoch 166/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4890 - val_loss: 0.4347\n",
      "Epoch 167/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4890 - val_loss: 0.4356\n",
      "Epoch 168/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4891 - val_loss: 0.4353\n",
      "Epoch 169/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4889 - val_loss: 0.4346\n",
      "Epoch 170/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4888 - val_loss: 0.4347\n",
      "Epoch 171/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4889 - val_loss: 0.4347\n",
      "Epoch 172/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4889 - val_loss: 0.4352\n",
      "Epoch 173/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4887 - val_loss: 0.4341\n",
      "Epoch 174/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4885 - val_loss: 0.4341\n",
      "Epoch 175/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4885 - val_loss: 0.4345\n",
      "Epoch 176/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4884 - val_loss: 0.4342\n",
      "Epoch 177/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4883 - val_loss: 0.4338\n",
      "Epoch 178/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4883 - val_loss: 0.4342\n",
      "Epoch 179/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4886 - val_loss: 0.4341\n",
      "Epoch 180/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4883 - val_loss: 0.4343\n",
      "Epoch 181/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4881 - val_loss: 0.4340\n",
      "Epoch 182/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4880 - val_loss: 0.4340\n",
      "Epoch 183/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4880 - val_loss: 0.4342\n",
      "Epoch 184/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4880 - val_loss: 0.4333\n",
      "Epoch 185/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4878 - val_loss: 0.4341\n",
      "Epoch 186/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4876 - val_loss: 0.4331\n",
      "Epoch 187/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4875 - val_loss: 0.4333\n",
      "Epoch 188/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4871 - val_loss: 0.4328\n",
      "Epoch 189/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4868 - val_loss: 0.4325\n",
      "Epoch 190/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4866 - val_loss: 0.4320\n",
      "Epoch 191/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4863 - val_loss: 0.4323\n",
      "Epoch 192/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4861 - val_loss: 0.4313\n",
      "Epoch 193/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4856 - val_loss: 0.4313\n",
      "Epoch 194/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4854 - val_loss: 0.4308\n",
      "Epoch 195/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4850 - val_loss: 0.4310\n",
      "Epoch 196/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4847 - val_loss: 0.4305\n",
      "Epoch 197/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4846 - val_loss: 0.4299\n",
      "Epoch 198/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4842 - val_loss: 0.4309\n",
      "Epoch 199/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4841 - val_loss: 0.4301\n",
      "Epoch 200/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4839 - val_loss: 0.4297\n",
      "Epoch 201/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4837 - val_loss: 0.4290\n",
      "Epoch 202/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4835 - val_loss: 0.4293\n",
      "Epoch 203/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4833 - val_loss: 0.4290\n",
      "Epoch 204/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4831 - val_loss: 0.4285\n",
      "Epoch 205/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4831 - val_loss: 0.4287\n",
      "Epoch 206/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4831 - val_loss: 0.4283\n",
      "Epoch 207/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4826 - val_loss: 0.4284\n",
      "Epoch 208/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4825 - val_loss: 0.4281\n",
      "Epoch 209/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4822 - val_loss: 0.4279\n",
      "Epoch 210/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4821 - val_loss: 0.4273\n",
      "Epoch 211/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4821 - val_loss: 0.4276\n",
      "Epoch 212/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4820 - val_loss: 0.4273\n",
      "Epoch 213/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4818 - val_loss: 0.4279\n",
      "Epoch 214/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4816 - val_loss: 0.4276\n",
      "Epoch 215/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4816 - val_loss: 0.4272\n",
      "Epoch 216/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4815 - val_loss: 0.4268\n",
      "Epoch 217/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4814 - val_loss: 0.4269\n",
      "Epoch 218/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4813 - val_loss: 0.4269\n",
      "Epoch 219/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4814 - val_loss: 0.4268\n",
      "Epoch 220/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4810 - val_loss: 0.4266\n",
      "Epoch 221/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4811 - val_loss: 0.4258\n",
      "Epoch 222/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4809 - val_loss: 0.4260\n",
      "Epoch 223/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4809 - val_loss: 0.4260\n",
      "Epoch 224/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4807 - val_loss: 0.4256\n",
      "Epoch 225/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4807 - val_loss: 0.4259\n",
      "Epoch 226/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4806 - val_loss: 0.4262\n",
      "Epoch 227/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4807 - val_loss: 0.4263\n",
      "Epoch 228/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4804 - val_loss: 0.4257\n",
      "Epoch 229/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4804 - val_loss: 0.4266\n",
      "Epoch 230/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4803 - val_loss: 0.4255\n",
      "Epoch 231/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4802 - val_loss: 0.4257\n",
      "Epoch 232/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4801 - val_loss: 0.4254\n",
      "Epoch 233/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4801 - val_loss: 0.4259\n",
      "Epoch 234/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4801 - val_loss: 0.4253\n",
      "Epoch 235/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4803 - val_loss: 0.4250\n",
      "Epoch 236/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4797 - val_loss: 0.4250\n",
      "Epoch 237/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4796 - val_loss: 0.4251\n",
      "Epoch 238/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4799 - val_loss: 0.4253\n",
      "Epoch 239/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4796 - val_loss: 0.4248\n",
      "Epoch 240/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4794 - val_loss: 0.4252\n",
      "Epoch 241/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4796 - val_loss: 0.4249\n",
      "Epoch 242/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4795 - val_loss: 0.4261\n",
      "Epoch 243/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4795 - val_loss: 0.4249\n",
      "Epoch 244/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4795 - val_loss: 0.4252\n",
      "Epoch 245/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4793 - val_loss: 0.4248\n",
      "Epoch 246/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4796 - val_loss: 0.4248\n",
      "Epoch 247/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4792 - val_loss: 0.4256\n",
      "Epoch 248/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4792 - val_loss: 0.4244\n",
      "Epoch 249/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4791 - val_loss: 0.4242\n",
      "Epoch 250/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4789 - val_loss: 0.4247\n",
      "Epoch 1/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.7587 - val_loss: 0.6555\n",
      "Epoch 2/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.6393 - val_loss: 0.6089\n",
      "Epoch 3/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.6039 - val_loss: 0.5849\n",
      "Epoch 4/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5847 - val_loss: 0.5691\n",
      "Epoch 5/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5708 - val_loss: 0.5570\n",
      "Epoch 6/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5601 - val_loss: 0.5483\n",
      "Epoch 7/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5526 - val_loss: 0.5413\n",
      "Epoch 8/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5467 - val_loss: 0.5364\n",
      "Epoch 9/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5424 - val_loss: 0.5327\n",
      "Epoch 10/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5390 - val_loss: 0.5297\n",
      "Epoch 11/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5363 - val_loss: 0.5271\n",
      "Epoch 12/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5339 - val_loss: 0.5246\n",
      "Epoch 13/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5316 - val_loss: 0.5228\n",
      "Epoch 14/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5297 - val_loss: 0.5210\n",
      "Epoch 15/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5279 - val_loss: 0.5188\n",
      "Epoch 16/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5263 - val_loss: 0.5177\n",
      "Epoch 17/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5247 - val_loss: 0.5160\n",
      "Epoch 18/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5230 - val_loss: 0.5144\n",
      "Epoch 19/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5213 - val_loss: 0.5126\n",
      "Epoch 20/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5195 - val_loss: 0.5110\n",
      "Epoch 21/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5177 - val_loss: 0.5094\n",
      "Epoch 22/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5158 - val_loss: 0.5076\n",
      "Epoch 23/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5140 - val_loss: 0.5058\n",
      "Epoch 24/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5125 - val_loss: 0.5048\n",
      "Epoch 25/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5111 - val_loss: 0.5030\n",
      "Epoch 26/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5097 - val_loss: 0.5019\n",
      "Epoch 27/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5077 - val_loss: 0.4989\n",
      "Epoch 28/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5059 - val_loss: 0.4975\n",
      "Epoch 29/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5044 - val_loss: 0.4958\n",
      "Epoch 30/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5030 - val_loss: 0.4943\n",
      "Epoch 31/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.5014 - val_loss: 0.4930\n",
      "Epoch 32/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4999 - val_loss: 0.4915\n",
      "Epoch 33/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4986 - val_loss: 0.4896\n",
      "Epoch 34/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4970 - val_loss: 0.4885\n",
      "Epoch 35/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4958 - val_loss: 0.4872\n",
      "Epoch 36/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4947 - val_loss: 0.4866\n",
      "Epoch 37/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4939 - val_loss: 0.4855\n",
      "Epoch 38/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4930 - val_loss: 0.4849\n",
      "Epoch 39/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4923 - val_loss: 0.4836\n",
      "Epoch 40/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4914 - val_loss: 0.4828\n",
      "Epoch 41/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4902 - val_loss: 0.4826\n",
      "Epoch 42/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4894 - val_loss: 0.4811\n",
      "Epoch 43/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4888 - val_loss: 0.4804\n",
      "Epoch 44/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4883 - val_loss: 0.4802\n",
      "Epoch 45/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4879 - val_loss: 0.4798\n",
      "Epoch 46/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4875 - val_loss: 0.4792\n",
      "Epoch 47/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4871 - val_loss: 0.4789\n",
      "Epoch 48/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4868 - val_loss: 0.4785\n",
      "Epoch 49/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4864 - val_loss: 0.4784\n",
      "Epoch 50/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4861 - val_loss: 0.4778\n",
      "Epoch 51/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4857 - val_loss: 0.4775\n",
      "Epoch 52/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4854 - val_loss: 0.4772\n",
      "Epoch 53/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4851 - val_loss: 0.4769\n",
      "Epoch 54/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4849 - val_loss: 0.4768\n",
      "Epoch 55/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4847 - val_loss: 0.4772\n",
      "Epoch 56/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4844 - val_loss: 0.4771\n",
      "Epoch 57/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4843 - val_loss: 0.4763\n",
      "Epoch 58/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4839 - val_loss: 0.4765\n",
      "Epoch 59/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4837 - val_loss: 0.4763\n",
      "Epoch 60/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4834 - val_loss: 0.4754\n",
      "Epoch 61/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4833 - val_loss: 0.4759\n",
      "Epoch 62/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4831 - val_loss: 0.4749\n",
      "Epoch 63/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4828 - val_loss: 0.4747\n",
      "Epoch 64/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4826 - val_loss: 0.4750\n",
      "Epoch 65/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4824 - val_loss: 0.4742\n",
      "Epoch 66/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4823 - val_loss: 0.4743\n",
      "Epoch 67/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4820 - val_loss: 0.4737\n",
      "Epoch 68/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4818 - val_loss: 0.4737\n",
      "Epoch 69/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4817 - val_loss: 0.4735\n",
      "Epoch 70/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4815 - val_loss: 0.4731\n",
      "Epoch 71/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4813 - val_loss: 0.4733\n",
      "Epoch 72/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4812 - val_loss: 0.4746\n",
      "Epoch 73/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4810 - val_loss: 0.4732\n",
      "Epoch 74/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4808 - val_loss: 0.4736\n",
      "Epoch 75/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4807 - val_loss: 0.4726\n",
      "Epoch 76/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4805 - val_loss: 0.4726\n",
      "Epoch 77/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4804 - val_loss: 0.4720\n",
      "Epoch 78/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4801 - val_loss: 0.4725\n",
      "Epoch 79/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4800 - val_loss: 0.4725\n",
      "Epoch 80/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4799 - val_loss: 0.4717\n",
      "Epoch 81/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4797 - val_loss: 0.4714\n",
      "Epoch 82/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4795 - val_loss: 0.4717\n",
      "Epoch 83/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4795 - val_loss: 0.4710\n",
      "Epoch 84/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4792 - val_loss: 0.4712\n",
      "Epoch 85/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4791 - val_loss: 0.4709\n",
      "Epoch 86/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4789 - val_loss: 0.4708\n",
      "Epoch 87/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4788 - val_loss: 0.4709\n",
      "Epoch 88/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4786 - val_loss: 0.4711\n",
      "Epoch 89/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4785 - val_loss: 0.4705\n",
      "Epoch 90/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4783 - val_loss: 0.4704\n",
      "Epoch 91/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4782 - val_loss: 0.4704\n",
      "Epoch 92/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4781 - val_loss: 0.4702\n",
      "Epoch 93/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4779 - val_loss: 0.4700\n",
      "Epoch 94/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4777 - val_loss: 0.4700\n",
      "Epoch 95/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4776 - val_loss: 0.4708\n",
      "Epoch 96/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4774 - val_loss: 0.4690\n",
      "Epoch 97/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4774 - val_loss: 0.4689\n",
      "Epoch 98/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4772 - val_loss: 0.4699\n",
      "Epoch 99/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4770 - val_loss: 0.4688\n",
      "Epoch 100/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4769 - val_loss: 0.4685\n",
      "Epoch 101/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4766 - val_loss: 0.4688\n",
      "Epoch 102/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4766 - val_loss: 0.4683\n",
      "Epoch 103/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4764 - val_loss: 0.4684\n",
      "Epoch 104/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4763 - val_loss: 0.4681\n",
      "Epoch 105/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4761 - val_loss: 0.4688\n",
      "Epoch 106/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4760 - val_loss: 0.4678\n",
      "Epoch 107/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4759 - val_loss: 0.4678\n",
      "Epoch 108/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4757 - val_loss: 0.4676\n",
      "Epoch 109/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4756 - val_loss: 0.4678\n",
      "Epoch 110/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4755 - val_loss: 0.4671\n",
      "Epoch 111/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4754 - val_loss: 0.4672\n",
      "Epoch 112/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4752 - val_loss: 0.4670\n",
      "Epoch 113/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4751 - val_loss: 0.4667\n",
      "Epoch 114/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4750 - val_loss: 0.4672\n",
      "Epoch 115/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4748 - val_loss: 0.4667\n",
      "Epoch 116/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4747 - val_loss: 0.4666\n",
      "Epoch 117/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4746 - val_loss: 0.4669\n",
      "Epoch 118/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4745 - val_loss: 0.4663\n",
      "Epoch 119/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4743 - val_loss: 0.4671\n",
      "Epoch 120/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4742 - val_loss: 0.4666\n",
      "Epoch 121/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4740 - val_loss: 0.4666\n",
      "Epoch 122/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4739 - val_loss: 0.4659\n",
      "Epoch 123/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4737 - val_loss: 0.4655\n",
      "Epoch 124/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4736 - val_loss: 0.4662\n",
      "Epoch 125/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4734 - val_loss: 0.4666\n",
      "Epoch 126/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4737 - val_loss: 0.4653\n",
      "Epoch 127/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4731 - val_loss: 0.4652\n",
      "Epoch 128/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4729 - val_loss: 0.4650\n",
      "Epoch 129/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4727 - val_loss: 0.4646\n",
      "Epoch 130/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4726 - val_loss: 0.4660\n",
      "Epoch 131/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4724 - val_loss: 0.4648\n",
      "Epoch 132/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4723 - val_loss: 0.4650\n",
      "Epoch 133/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4721 - val_loss: 0.4646\n",
      "Epoch 134/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4719 - val_loss: 0.4642\n",
      "Epoch 135/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4718 - val_loss: 0.4638\n",
      "Epoch 136/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4716 - val_loss: 0.4638\n",
      "Epoch 137/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4715 - val_loss: 0.4633\n",
      "Epoch 138/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4714 - val_loss: 0.4635\n",
      "Epoch 139/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4713 - val_loss: 0.4635\n",
      "Epoch 140/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4711 - val_loss: 0.4635\n",
      "Epoch 141/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4709 - val_loss: 0.4638\n",
      "Epoch 142/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4709 - val_loss: 0.4630\n",
      "Epoch 143/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4708 - val_loss: 0.4630\n",
      "Epoch 144/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4706 - val_loss: 0.4631\n",
      "Epoch 145/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4705 - val_loss: 0.4630\n",
      "Epoch 146/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4705 - val_loss: 0.4629\n",
      "Epoch 147/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4703 - val_loss: 0.4627\n",
      "Epoch 148/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4703 - val_loss: 0.4627\n",
      "Epoch 149/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4701 - val_loss: 0.4624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4700 - val_loss: 0.4625\n",
      "Epoch 151/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4698 - val_loss: 0.4628\n",
      "Epoch 152/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4698 - val_loss: 0.4623\n",
      "Epoch 153/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4695 - val_loss: 0.4620\n",
      "Epoch 154/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4694 - val_loss: 0.4616\n",
      "Epoch 155/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4694 - val_loss: 0.4610\n",
      "Epoch 156/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4693 - val_loss: 0.4618\n",
      "Epoch 157/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4693 - val_loss: 0.4615\n",
      "Epoch 158/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4691 - val_loss: 0.4610\n",
      "Epoch 159/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4690 - val_loss: 0.4618\n",
      "Epoch 160/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4689 - val_loss: 0.4619\n",
      "Epoch 161/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4689 - val_loss: 0.4620\n",
      "Epoch 162/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4688 - val_loss: 0.4615\n",
      "Epoch 163/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4687 - val_loss: 0.4610\n",
      "Epoch 164/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4686 - val_loss: 0.4613\n",
      "Epoch 165/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4684 - val_loss: 0.4607\n",
      "Epoch 166/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4685 - val_loss: 0.4615\n",
      "Epoch 167/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4685 - val_loss: 0.4614\n",
      "Epoch 168/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4683 - val_loss: 0.4608\n",
      "Epoch 169/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4682 - val_loss: 0.4609\n",
      "Epoch 170/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4682 - val_loss: 0.4599\n",
      "Epoch 171/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4681 - val_loss: 0.4601\n",
      "Epoch 172/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4679 - val_loss: 0.4607\n",
      "Epoch 173/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4679 - val_loss: 0.4605\n",
      "Epoch 174/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4678 - val_loss: 0.4605\n",
      "Epoch 175/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4678 - val_loss: 0.4604\n",
      "Epoch 176/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4678 - val_loss: 0.4607\n",
      "Epoch 177/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4676 - val_loss: 0.4599\n",
      "Epoch 178/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4675 - val_loss: 0.4603\n",
      "Epoch 179/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4676 - val_loss: 0.4600\n",
      "Epoch 180/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4674 - val_loss: 0.4598\n",
      "Epoch 181/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4674 - val_loss: 0.4596\n",
      "Epoch 182/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4674 - val_loss: 0.4602\n",
      "Epoch 183/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4673 - val_loss: 0.4606\n",
      "Epoch 184/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4671 - val_loss: 0.4599\n",
      "Epoch 185/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4672 - val_loss: 0.4597\n",
      "Epoch 186/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4671 - val_loss: 0.4593\n",
      "Epoch 187/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4670 - val_loss: 0.4599\n",
      "Epoch 188/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4669 - val_loss: 0.4593\n",
      "Epoch 189/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4669 - val_loss: 0.4589\n",
      "Epoch 190/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4668 - val_loss: 0.4589\n",
      "Epoch 191/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4669 - val_loss: 0.4590\n",
      "Epoch 192/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4668 - val_loss: 0.4604\n",
      "Epoch 193/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4668 - val_loss: 0.4595\n",
      "Epoch 194/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4666 - val_loss: 0.4604\n",
      "Epoch 195/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4665 - val_loss: 0.4587\n",
      "Epoch 196/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4666 - val_loss: 0.4593\n",
      "Epoch 197/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4666 - val_loss: 0.4586\n",
      "Epoch 198/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4664 - val_loss: 0.4588\n",
      "Epoch 199/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4664 - val_loss: 0.4589\n",
      "Epoch 200/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4663 - val_loss: 0.4588\n",
      "Epoch 201/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4663 - val_loss: 0.4590\n",
      "Epoch 202/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4663 - val_loss: 0.4580\n",
      "Epoch 203/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4662 - val_loss: 0.4593\n",
      "Epoch 204/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4662 - val_loss: 0.4586\n",
      "Epoch 205/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4661 - val_loss: 0.4583\n",
      "Epoch 206/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4660 - val_loss: 0.4583\n",
      "Epoch 207/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4661 - val_loss: 0.4586\n",
      "Epoch 208/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4660 - val_loss: 0.4580\n",
      "Epoch 209/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4659 - val_loss: 0.4586\n",
      "Epoch 210/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4659 - val_loss: 0.4578\n",
      "Epoch 211/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4659 - val_loss: 0.4582\n",
      "Epoch 212/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4658 - val_loss: 0.4590\n",
      "Epoch 213/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4659 - val_loss: 0.4577\n",
      "Epoch 214/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4657 - val_loss: 0.4586\n",
      "Epoch 215/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4656 - val_loss: 0.4580\n",
      "Epoch 216/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4655 - val_loss: 0.4579\n",
      "Epoch 217/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4657 - val_loss: 0.4588\n",
      "Epoch 218/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4656 - val_loss: 0.4579\n",
      "Epoch 219/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4654 - val_loss: 0.4579\n",
      "Epoch 220/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4655 - val_loss: 0.4579\n",
      "Epoch 221/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4653 - val_loss: 0.4582\n",
      "Epoch 222/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4653 - val_loss: 0.4587\n",
      "Epoch 223/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4653 - val_loss: 0.4575\n",
      "Epoch 224/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4651 - val_loss: 0.4579\n",
      "Epoch 225/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4651 - val_loss: 0.4574\n",
      "Epoch 226/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4651 - val_loss: 0.4578\n",
      "Epoch 227/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4650 - val_loss: 0.4573\n",
      "Epoch 228/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4650 - val_loss: 0.4569\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4649 - val_loss: 0.4571\n",
      "Epoch 230/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4648 - val_loss: 0.4576\n",
      "Epoch 231/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4649 - val_loss: 0.4569\n",
      "Epoch 232/250\n",
      "492/492 [==============================] - 1s 2ms/step - loss: 0.4649 - val_loss: 0.4572\n",
      "Epoch 233/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4648 - val_loss: 0.4573\n",
      "Epoch 234/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4647 - val_loss: 0.4583\n",
      "Epoch 235/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4647 - val_loss: 0.4580\n",
      "Epoch 236/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4646 - val_loss: 0.4565\n",
      "Epoch 237/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4645 - val_loss: 0.4565\n",
      "Epoch 238/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4644 - val_loss: 0.4569\n",
      "Epoch 239/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4645 - val_loss: 0.4580\n",
      "Epoch 240/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4644 - val_loss: 0.4564\n",
      "Epoch 241/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4643 - val_loss: 0.4569\n",
      "Epoch 242/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4645 - val_loss: 0.4568\n",
      "Epoch 243/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4643 - val_loss: 0.4564\n",
      "Epoch 244/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4643 - val_loss: 0.4564\n",
      "Epoch 245/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4642 - val_loss: 0.4563\n",
      "Epoch 246/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4641 - val_loss: 0.4564\n",
      "Epoch 247/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4642 - val_loss: 0.4568\n",
      "Epoch 248/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4641 - val_loss: 0.4560\n",
      "Epoch 249/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4640 - val_loss: 0.4562\n",
      "Epoch 250/250\n",
      "492/492 [==============================] - 1s 1ms/step - loss: 0.4641 - val_loss: 0.4564\n"
     ]
    }
   ],
   "source": [
    "# Use an autoencoder to transform the raw data before fitting the model\n",
    "classifier, (x_test, y_test) = train_on_encrypted_data(data_alice, data_bob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with independently encoded data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[1246    1]\n",
      " [  21   85]]\n",
      "Associated quality metrics :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1247\n",
      "           1       0.99      0.80      0.89       106\n",
      "\n",
      "    accuracy                           0.98      1353\n",
      "   macro avg       0.99      0.90      0.94      1353\n",
      "weighted avg       0.98      0.98      0.98      1353\n",
      "\n",
      "AUC score :\n",
      "0.9005424339168722\n"
     ]
    }
   ],
   "source": [
    "verif_valid(classifier, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Train the classifier on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an autoencoder to transform the raw data before fitting the model\n",
    "classifier, (x_test, y_test) = train_on_raw_data(data_alice, data_bob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[1245    2]\n",
      " [  14   92]]\n",
      "Associated quality metrics :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1247\n",
      "           1       0.98      0.87      0.92       106\n",
      "\n",
      "    accuracy                           0.99      1353\n",
      "   macro avg       0.98      0.93      0.96      1353\n",
      "weighted avg       0.99      0.99      0.99      1353\n",
      "\n",
      "AUC score :\n",
      "0.9331603395318576\n"
     ]
    }
   ],
   "source": [
    "verif_valid(classifier, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}